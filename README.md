# SummariseIt üéôÔ∏è

**SummariseIt** allows you to summarise your beloved podcast whenever you are not in the mood to listen to it, or when you want to learn from its insights but rather not spend your precious time consuming the content. It provides a high-level overview, along with the key takeaways and lessons. 

<div align="center">
   <img src="assets/summariseit.gif" width="90%" max-width="500" style="border-radius:1%"/>
</div>

## Features
- **Automated Summarisation**: This project generates a concise summary of your desired podcast using your desired LLM provider and model.
- **Efficient Summarisation**: We divide the transcript of the podcast into chunks and summarise each subsequent chunk asynchronously. allowing podcasts of over 1 hour long to be summarised in less than 30 seconds.
- **Trascription using [Whisper](https://openai.com/index/whisper/)**: When the podcast does not have a transcript, we employ Whisper to transcribe the podcast.
- **LLM Factory**: The setup allows you to easily switch between different LLM providers and models.
- **[Instructor](https://python.useinstructor.com/)**: Built on top of Pydantic, integration with Instructor allows for structured output and data model validation under the hood.


## Getting Started
1. Clone the repository
    
   ```
   git clone https://github.com/CodeGrinder96/summariseit.git
   cd summariseit
   ```

2. Create an environment file with the api key corresponding to your LLM of choice, according to `.env.example`

    - The current implementation supports: `openai, azure_openai, anthropic, llama`
    - If you wish to use a local model, start by downloading [Ollama](https://ollama.com/download). Then follow the instructions to pull and serve your model. You may want to edit `src/utils/summariser_utils.py` in case you wish to use another model than the default `llama3`. 

3. Run the app using [Docker](https://docs.docker.com/get-started/)

    ```
    docker compose up --build
    ```
4. Your local app is now accessible at `localhost:8501`